{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mirek_Assignment1_dl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXgWugfJ0Vl"
      },
      "source": [
        "## Assignment 1\n",
        "\n",
        "**Submission deadlines:** \n",
        "- get at least **2** points by Tuesday, 9.03.2021\n",
        "- remaining points: last lab session before or on Tuesday, 16.03.2021\n",
        "\n",
        "**Points:** Aim to get 8 out of 12 possible points\n",
        "\n",
        "## Submission instructions\n",
        "The class is held remotely. To sumbmit your solutions please show the notebook over the video call. Make sure you know all the questions and asnwers, and that the notebook contains results (before presentation do `Runtime -> Restar and run all`)\n",
        "\n",
        "We provide starter code, however you are not required to use it as long as you properly solve the tasks.\n",
        "\n",
        "As always, please submit corrections using GitHub's Pull Requests to https://github.com/janchorowski/dl_uwr."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S8iRaCPyO2a"
      },
      "source": [
        "# Task description\r\n",
        "\r\n",
        "## TLDR\r\n",
        "Implement and train a neural network using pure numpy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHcz7I2V-bVM"
      },
      "source": [
        "\r\n",
        "## Problem 1 [2p]\r\n",
        "Implement a two-layer network, manually set weights and biases to solve the XOR task.\r\n",
        "\r\n",
        "A two-layer neural network implementes a function $f: \\mathbb{R}^D \\rightarrow \\mathbb{R}^O$ where $D$ is the input dimensionality and $O$ is the output dinemsionality. The output goes through an intermediate representation (the hidden layer) with dimensionality $H$. \r\n",
        "\r\n",
        "The computations are as follows:\r\n",
        "\\begin{equation}\r\n",
        "\\begin{split}\r\n",
        "A_1 &= x W_1^T + b_1  & \\qquad\\text{Total input to neurons in the hidden layer (network's first layer)} \\\\\r\n",
        "O_1 &= \\sigma_1(A_1)  & \\qquad\\text{Output of the hidden layer} \\\\\r\n",
        "A_2 &= O_1 W_2^T + b_2 & \\qquad\\text{Total input to neurons in the output layer (network's second layer)}\\\\\r\n",
        "O_2 &= \\sigma_2(A_2) & \\qquad\\text{Output of the network}\r\n",
        "\\end{split}\r\n",
        "\\end{equation}\r\n",
        "\r\n",
        "Where $W$ are weight matrices, $b$ are bias vectors, $\\sigma$ are non-linear activation functions (e.g. the logistic sigmoid applied element-wise, or softmax).\r\n",
        "\r\n",
        "For the 2D xor problem the network will:\r\n",
        "- have 2 inputs, 2 hidden neurons, one output\r\n",
        "- use the logistic sigmoid everywhere (that way we, when hand-designig the weights, we can assume that neurons' outputs are binary).\r\n",
        "\r\n",
        "Therrefore the shapes of the data floing through the network will be:\r\n",
        "- input: $x\\in\\mathbb{}R^{2}$\r\n",
        "- hidden layer parameters: $W_1\\in\\mathbb{}R^{2\\times 2}$ and $b_1\\in\\mathbb{}R^{2}$\r\n",
        "- representations in the hidden layer: $A_1\\in\\mathbb{}R^{2}$ and $O_1\\in\\mathbb{}R^{2}$\r\n",
        "- output layer parameters: $W_2\\in\\mathbb{}R^{1\\times 2}$ and $b_1\\in\\mathbb{}R^{1}$\r\n",
        "- representations in the output layer: $A_2\\in\\mathbb{}R^{1}$ and $O_2\\in\\mathbb{}R^{1}$\r\n",
        "\r\n",
        "The network can be seen as a logistic regression model, prefixed by a nonlinear transformation of the data.\r\n",
        "\r\n",
        "The first tasks consists of:\r\n",
        "- implementing the network\r\n",
        "- selecting parametwrs ($W_1, b_1, W_2, b_2$) such that $f(x)\\approx XOR(x_1, x_2)$ where the approximation is die to the sigmoids - the output may be close to 0 or 1, but doesn't need to saturate at 0 or 1.\r\n",
        "\r\n",
        "NB: the convention on weight matrix shapes follows linear [layers in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QSpZxuH-bLe"
      },
      "source": [
        "## Problem 2 [2p]\r\n",
        "1. Add a backward pass.\r\n",
        "2. Use a sensible random initialization for weights and biases.\r\n",
        "3. Numerically check the correctness of your gradient computation.\r\n",
        "\r\n",
        "There is nice article about taking derivative over vectors and vector chain rule: https://explained.ai/matrix-calculus/ if someone don't have experience with suchr calculus.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Tn8j0m-bAy"
      },
      "source": [
        "## Problem 3 [2p]\r\n",
        "1. Implement gradient descent\r\n",
        "2. Train your network to solve 3D XOR\r\n",
        "3. Try several hidden layer sizes, for each size record the fracton of successful trainings. Then answer:\r\n",
        "    - What is the minimal hidden size required to solve 3D XOR (even with low reliability, when the training has to be repeated multiple times)\r\n",
        "    - What is the minimal hidden size required to reliably solve 3D XOR\r\n",
        "    - Which networks are easier to train - small or large ones? Why?\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP9Pvpmf-a2A"
      },
      "source": [
        "## Problem 4 [1p]\r\n",
        "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Find a network architecture which reliably learns the 3D XOR problem.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGgtpe-w-asB"
      },
      "source": [
        "## Problem 5 [1p]\r\n",
        "Add a second hidden layer to your network, implement the forward and backward pass, then demonstrate training.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe-pcFeO-aiE"
      },
      "source": [
        "## Problem 6 [2p]\r\n",
        "Implement a way to have a _variable number_ of hidden layers. Check how deep sigmoid or ReLU networks you  can train. For simplicity you can assume that all hidden layers have the same number of neurons, and use the same activation function.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIpn17Cm-aW7"
      },
      "source": [
        "## Problem 7 [2p]\r\n",
        "For each weight matrix $w\\in\\mathbb{R}^{n\\times m}$, add a randomly initialized `backward weight` $w_b\\in\\mathbb{R}^{m\\times n}$, which will not change during training. Change the backward pass to use $w_b$ instead of $w^T$, getting an approxmatoin of the true gradient. Can you get your network to train?\r\n",
        "\r\n",
        "NB: this approach, dubbed [feedback alignment](https://www.nature.com/articles/ncomms13276), was proposed to make error backpropagation more biologically plausible, by providing a solution to the \"weight transport problem\". Regular backpropagation requires that neurons not only know their incoming weights (thet they control), but also their outgoing weights (that are controlled by neurons in the upper layers). This is nearly impossible in a real brain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXJaoHSH0DZO"
      },
      "source": [
        "# Solutions and starter code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiTEWD2oqW0Y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqtfJKR40J3x"
      },
      "source": [
        "XOR dataset creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "lYEbCfbSpv5M",
        "outputId": "26274394-a881-4322-dcb4-db7b42956209"
      },
      "source": [
        "# Let's define a XOR dataset\n",
        "\n",
        "# X will be matrix of N 2-dimensional inputs\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1],], dtype=np.float32)\n",
        "# Y is a matrix of N numners - answers\n",
        "Y = np.array([[0], [1], [1], [0],], dtype=np.float32)\n",
        "\n",
        "plt.scatter(\n",
        "    X[:, 0], X[:, 1], c=Y[:, 0],\n",
        ")\n",
        "plt.xlabel(\"X[0]\")\n",
        "plt.ylabel(\"X[1]\")\n",
        "plt.axis(\"square\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.05, 1.05, -0.05, 1.05)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb3azMn929_I"
      },
      "source": [
        "## Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZCM_hdELE04"
      },
      "source": [
        "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
        "\n",
        "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
        "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
        "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
        "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D$ matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrrRuk6zLiF0"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+ np.exp(-x))\n",
        "\n",
        "def _delsigmoid(x):\n",
        " \n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "class SmallNet:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        # TODO for Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.uniform(low=0, high=0.5, size=(self.W1.shape))\n",
        "        # print(self.W1, \"init_params\")\n",
        "        self.b1 = np.random.uniform(low=0, high=0.5, size=(self.b1.shape))\n",
        "        self.W2 = np.random.uniform(low=0, high=0.5, size=(self.W2.shape))\n",
        "        self.b2 = np.random.uniform(low=0, high=0.5, size=(self.b2.shape))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "\n",
        "        # Input to neurons in 1st layer x W_1^T + b_1\n",
        "        # print(X, \"X\")\n",
        "        # print(self.W1.T, \"W1.T\")\n",
        "        # A1 = np.add(np.dot(X, self.W1.T), self.b1) \n",
        "        A1 = np.dot(X, self.W1.T) + self.b1\n",
        "        # print(A1, \"A1\")\n",
        "        # print(A1)\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O1 = sigmoid(A1)\n",
        "        # print(O1.shape)\n",
        "        # Inputs to neuron in the second layer\n",
        "        # A_2 &= O_1 W_2^T + b_2 \n",
        "        # print(self.W2.T, \"SELF.W.T\")\n",
        "        A2 = np.add(np.dot(O1, self.W2.T), self.b2)  \n",
        "        # print(A2, \"A2\")\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        # O_2 &= \\sigma_2(A_2)\n",
        "        O2 = sigmoid(A2)\n",
        "     \n",
        "        # print(\"Most important thing O2\",O2)\n",
        "        \n",
        "\n",
        "\n",
        "        # When Y is none, simply return the predictions. Else compute the loss\n",
        "        if Y is not None:   \n",
        "            loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n",
        "            # normalize loss by batch size\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "            # print(loss)\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            \n",
        "            A2_grad = O2 - Y\n",
        "            # print(A2_grad, \"Wynik ostatniej aktywacji - dokładna wartosc\")       \n",
        "            self.b2_grad = A2_grad.sum(0) / A2.shape[0]\n",
        "            self.W2_grad = np.dot(A2_grad.T, O1) / A2.shape[0]\n",
        "            O1_grad = np.dot(A2_grad, self.W2)\n",
        "            A1_grad = O1_grad * _delsigmoid(O1)\n",
        "            self.b1_grad = A1_grad.sum(0) / A2.shape[0]\n",
        "            self.W1_grad = A1_grad.T @ X /A2.shape[0]\n",
        "\n",
        "        return O2, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJswvBk0oiIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24f3d1e-f1fb-4bf3-e8db-d98b5c4d14f0"
      },
      "source": [
        "# TODO Problem 1:\n",
        "# Set by hand the weight values to solve the XOR problem\n",
        "\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "net.W1 = np.array([[10,-100],[10, -90]], dtype=np.float64)\n",
        "net.b1 = np.array([-10,90], dtype=np.float64)\n",
        "net.W2 = np.array([20, -15],dtype=np.float64)\n",
        "net.b2 = np.array(10,dtype=np.float64)\n",
        "# # Hint: since we use the logistic sigmoid activation, the weights may need to\n",
        "# # be fairly large\n",
        "\n",
        "predictions, loss = net.forward(X, Y, do_backward=False)\n",
        "\n",
        "for x, p in zip(X, predictions):\n",
        "    print(f\"XORnet({x}) = {p}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XORnet([0. 0.]) = 0.006698889780762399\n",
            "XORnet([0. 1.]) = 0.9241418199787566\n",
            "XORnet([1. 0.]) = 0.9933071490757153\n",
            "XORnet([1. 1.]) = 0.006697379559455959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmxCi5Vl6_xB"
      },
      "source": [
        "## Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSM5hgJ1mrhY"
      },
      "source": [
        "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
        "    \"\"\"A gradient checking routine\"\"\"\n",
        "\n",
        "    param = getattr(net, param_name)\n",
        "    param_flat_accessor = param.reshape(-1)\n",
        "\n",
        "\n",
        "    grad = np.empty_like(param)\n",
        "    grad_flat_accessor = grad.reshape(-1)\n",
        "\n",
        "\n",
        "    net.forward(X, Y, do_backward=True)\n",
        "    orig_grad = getattr(net, param_name + \"_grad\")\n",
        "    # print(orig_grad)\n",
        "\n",
        "    assert param.shape == orig_grad.shape\n",
        "\n",
        "    for i in range(param_flat_accessor.shape[0]):\n",
        "        orig_val = param_flat_accessor[i]\n",
        "        param_flat_accessor[i] = orig_val + eps\n",
        "        _, loss_positive = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val - eps\n",
        "        _, loss_negative = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val\n",
        "        grad_flat_accessor[i] = (loss_positive - loss_negative) / (2 * eps)\n",
        "    assert np.allclose(grad, orig_grad)\n",
        "    return grad, orig_grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTZu0jFEvgXF"
      },
      "source": [
        "# Hint: use float64 for checking the correctness of the gradient\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "\n",
        "for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "    check_grad(net, param_name, X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUOs3cVvjM2"
      },
      "source": [
        "## Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn2AAoZo0vjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a44565b3-7262-4d27-bf6d-dedd03dd607e"
      },
      "source": [
        "net = SmallNet(2, 10, dtype=np.float64)\n",
        "\n",
        "# Od alphy = 2 w góre różnice są minimalne \n",
        "alpha = 2  # set a learning rate\n",
        "\n",
        "for i in range(100000):\n",
        "    _, loss = net.forward(X, Y, do_backward=True)\n",
        "    if (i % 5000) == 0:\n",
        "        print(f\"after {i} steps \\tloss={loss}\")\n",
        "    for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "        param = getattr(net, param_name)\n",
        "        # print(param)\n",
        "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "        # Doing instead `param = new_val` simply changes to what the variable\n",
        "        # param points to, without affecting the network!\n",
        "        # alternatively, you could do setattr(net, param_name, new_value)\n",
        "        param[:] = param - alpha * getattr(net, param_name + \"_grad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after 0 steps \tloss=1.2120571624234557\n",
            "after 5000 steps \tloss=0.0007366242865105278\n",
            "after 10000 steps \tloss=0.00032390922586611655\n",
            "after 15000 steps \tloss=0.00020413815928333866\n",
            "after 20000 steps \tloss=0.00014790265395153677\n",
            "after 25000 steps \tloss=0.00011544522770083843\n",
            "after 30000 steps \tloss=9.439025382038474e-05\n",
            "after 35000 steps \tloss=7.966012677835609e-05\n",
            "after 40000 steps \tloss=6.879430140045229e-05\n",
            "after 45000 steps \tloss=6.0458320464431694e-05\n",
            "after 50000 steps \tloss=5.386684965827468e-05\n",
            "after 55000 steps \tloss=4.8528083426517614e-05\n",
            "after 60000 steps \tloss=4.4118585690352066e-05\n",
            "after 65000 steps \tloss=4.041705962741935e-05\n",
            "after 70000 steps \tloss=3.72670600360252e-05\n",
            "after 75000 steps \tloss=3.4554907527504346e-05\n",
            "after 80000 steps \tloss=3.219604076226376e-05\n",
            "after 85000 steps \tloss=3.0126274369969225e-05\n",
            "after 90000 steps \tloss=2.8296023605569686e-05\n",
            "after 95000 steps \tloss=2.6666385188148696e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwpEjpkU1JvK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95247030-e08a-4d2f-a95b-8ade29ba14e5"
      },
      "source": [
        "predictions, loss = net.forward(X, Y, do_backward=True)\n",
        "for x, p in zip(X, predictions):\n",
        "    print(f\"XORnet({x}) = {p[0]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XORnet([0. 0.]) = 3.4456465715732075e-05\n",
            "XORnet([0. 1.]) = 0.9999762248621572\n",
            "XORnet([1. 0.]) = 0.99997609490811\n",
            "XORnet([1. 1.]) = 1.868762556965595e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0ZMyHqz8xrC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae609dc-ae7e-4d3f-88c7-09a499b3c404"
      },
      "source": [
        "# TODO:\n",
        "# Generate data for a 3D XOR task\n",
        "# Then estimate the success rate of training the network with diferent\n",
        "# hidden sizes.\n",
        "# Y is a matrix of N numners - answers\n",
        "X3 = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1]], dtype=np.float32)\n",
        "Y3 =  np.array([[0], [1], [1], [1], [0], [0], [0], [1]], dtype=np.float32)\n",
        "# [2, 3, 5, 10, 20]\n",
        "for hidden_dim in [2, 3, 5, 10, 20]:\n",
        "    # TODO: run a fe trainings and record the fraction of successful ones\n",
        "    net = SmallNet(3, hidden_dim, dtype=np.float32)\n",
        "    alpha = 3  # set a learning rate\n",
        "    prev_loss = 1\n",
        "    for i in range(100000):\n",
        "        _, loss = net.forward(X3, Y3, do_backward=True)\n",
        "        if i % 5000 == 0:\n",
        "            print(f\"after {i} steps \\tloss={loss} neuron size {hidden_dim}\")\n",
        "            print(prev_loss - loss)\n",
        "            prev_loss = loss\n",
        "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\n",
        "            param = getattr(net, param_name)\n",
        "            # print(param)\n",
        "            # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "            # Doing instead `param = new_val` simply changes to what the variable\n",
        "            # param points to, without affecting the network!\n",
        "            # alternatively, you could do setattr(net, param_name, new_value)\n",
        "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n",
        "        \n",
        "        \n",
        "# Wartość pod print'em print(f\"after {i} steps \\tloss={loss} neuron size {hidden_dim}\") \n",
        "# oznacza skok dokładnosci "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after 0 steps \tloss=0.75165526851276 neuron size 2\n",
            "0.24834473148724\n",
            "after 5000 steps \tloss=0.016361774114557463 neuron size 2\n",
            "0.7352934943982026\n",
            "after 10000 steps \tloss=0.0046608631217060215 neuron size 2\n",
            "0.011700910992851442\n",
            "after 15000 steps \tloss=0.0028423343917655915 neuron size 2\n",
            "0.00181852872994043\n",
            "after 20000 steps \tloss=0.002045672069385124 neuron size 2\n",
            "0.0007966623223804674\n",
            "after 25000 steps \tloss=0.00159766404043446 neuron size 2\n",
            "0.0004480080289506641\n",
            "after 30000 steps \tloss=0.0013105196785648363 neuron size 2\n",
            "0.0002871443618696237\n",
            "after 35000 steps \tloss=0.0011108026443276788 neuron size 2\n",
            "0.00019971703423715746\n",
            "after 40000 steps \tloss=0.0009638701325131071 neuron size 2\n",
            "0.00014693251181457174\n",
            "after 45000 steps \tloss=0.0008512444735635077 neuron size 2\n",
            "0.00011262565894959941\n",
            "after 50000 steps \tloss=0.0007621698644900562 neuron size 2\n",
            "8.907460907345147e-05\n",
            "after 55000 steps \tloss=0.0006899605338679415 neuron size 2\n",
            "7.22093306221147e-05\n",
            "after 60000 steps \tloss=0.000630242169270775 neuron size 2\n",
            "5.971836459716653e-05\n",
            "after 65000 steps \tloss=0.0005800325717096835 neuron size 2\n",
            "5.020959756109149e-05\n",
            "after 70000 steps \tloss=0.0005372287335472076 neuron size 2\n",
            "4.280383816247584e-05\n",
            "after 75000 steps \tloss=0.0005003051176068338 neuron size 2\n",
            "3.6923615940373804e-05\n",
            "after 80000 steps \tloss=0.0004681281998664992 neuron size 2\n",
            "3.21769177403346e-05\n",
            "after 85000 steps \tloss=0.0004398381655794368 neuron size 2\n",
            "2.8290034287062435e-05\n",
            "after 90000 steps \tloss=0.0004147710063173273 neuron size 2\n",
            "2.5067159262109505e-05\n",
            "after 95000 steps \tloss=0.00039240578921029967 neuron size 2\n",
            "2.2365217107027607e-05\n",
            "after 0 steps \tloss=0.784492766416105 neuron size 3\n",
            "0.215507233583895\n",
            "after 5000 steps \tloss=0.00105755212376369 neuron size 3\n",
            "0.7834352142923413\n",
            "after 10000 steps \tloss=0.00046444268506979 neuron size 3\n",
            "0.0005931094386939001\n",
            "after 15000 steps \tloss=0.00029866542862379394 neuron size 3\n",
            "0.00016577725644599604\n",
            "after 20000 steps \tloss=0.00022039420583603263 neuron size 3\n",
            "7.827122278776131e-05\n",
            "after 25000 steps \tloss=0.00017474316395085706 neuron size 3\n",
            "4.565104188517557e-05\n",
            "after 30000 steps \tloss=0.00014481357510476672 neuron size 3\n",
            "2.992958884609034e-05\n",
            "after 35000 steps \tloss=0.00012366731621645641 neuron size 3\n",
            "2.1146258888310302e-05\n",
            "after 40000 steps \tloss=0.00010792790518809455 neuron size 3\n",
            "1.5739411028361865e-05\n",
            "after 45000 steps \tloss=9.57541019028701e-05 neuron size 3\n",
            "1.217380328522445e-05\n",
            "after 50000 steps \tloss=8.605603665518566e-05 neuron size 3\n",
            "9.69806524768444e-06\n",
            "after 55000 steps \tloss=7.81472533343327e-05 neuron size 3\n",
            "7.908783320852958e-06\n",
            "after 60000 steps \tloss=7.157378310613266e-05 neuron size 3\n",
            "6.573470228200039e-06\n",
            "after 65000 steps \tloss=6.602336086652976e-05 neuron size 3\n",
            "5.5504222396029e-06\n",
            "after 70000 steps \tloss=6.127411294196703e-05 neuron size 3\n",
            "4.749247924562734e-06\n",
            "after 75000 steps \tloss=5.716404272548385e-05 neuron size 3\n",
            "4.110070216483174e-06\n",
            "after 80000 steps \tloss=5.3572099579294586e-05 neuron size 3\n",
            "3.591943146189267e-06\n",
            "after 85000 steps \tloss=5.040600523858403e-05 neuron size 3\n",
            "3.1660943407105534e-06\n",
            "after 90000 steps \tloss=4.759418097238493e-05 neuron size 3\n",
            "2.8118242661991024e-06\n",
            "after 95000 steps \tloss=4.508024921227494e-05 neuron size 3\n",
            "2.5139317601099938e-06\n",
            "after 0 steps \tloss=0.837046017081673 neuron size 5\n",
            "0.16295398291832697\n",
            "after 5000 steps \tloss=0.00044578769131426615 neuron size 5\n",
            "0.8366002293903587\n",
            "after 10000 steps \tloss=0.00017242863954136645 neuron size 5\n",
            "0.0002733590517728997\n",
            "after 15000 steps \tloss=0.00010550728254702013 neuron size 5\n",
            "6.692135699434632e-05\n",
            "after 20000 steps \tloss=7.567344666349924e-05 neuron size 5\n",
            "2.9833835883520885e-05\n",
            "after 25000 steps \tloss=5.886736786917319e-05 neuron size 5\n",
            "1.6806078794326052e-05\n",
            "after 30000 steps \tloss=4.811121861625458e-05 neuron size 5\n",
            "1.0756149252918608e-05\n",
            "after 35000 steps \tloss=4.064728535990841e-05 neuron size 5\n",
            "7.4639332563461715e-06\n",
            "after 40000 steps \tloss=3.516983862085955e-05 neuron size 5\n",
            "5.477446739048859e-06\n",
            "after 45000 steps \tloss=3.098172230412182e-05 neuron size 5\n",
            "4.18811631673773e-06\n",
            "after 50000 steps \tloss=2.767721248766461e-05 neuron size 5\n",
            "3.3045098164572126e-06\n",
            "after 55000 steps \tloss=2.500432485968297e-05 neuron size 5\n",
            "2.6728876279816387e-06\n",
            "after 60000 steps \tloss=2.2798385127706237e-05 neuron size 5\n",
            "2.2059397319767338e-06\n",
            "after 65000 steps \tloss=2.094727519818179e-05 neuron size 5\n",
            "1.851109929524447e-06\n",
            "after 70000 steps \tloss=1.937204169008955e-05 neuron size 5\n",
            "1.5752335080922408e-06\n",
            "after 75000 steps \tloss=1.8015492835534365e-05 neuron size 5\n",
            "1.3565488545551835e-06\n",
            "after 80000 steps \tloss=1.683519351161834e-05 neuron size 5\n",
            "1.1802993239160243e-06\n",
            "after 85000 steps \tloss=1.5799000118603517e-05 neuron size 5\n",
            "1.0361933930148235e-06\n",
            "after 90000 steps \tloss=1.4882122833058055e-05 neuron size 5\n",
            "9.168772855454623e-07\n",
            "after 95000 steps \tloss=1.4065138954758614e-05 neuron size 5\n",
            "8.169838782994415e-07\n",
            "after 0 steps \tloss=0.9512132275432339 neuron size 10\n",
            "0.0487867724567661\n",
            "after 5000 steps \tloss=0.0006791792036916339 neuron size 10\n",
            "0.9505340483395422\n",
            "after 10000 steps \tloss=0.0002182505854560765 neuron size 10\n",
            "0.0004609286182355574\n",
            "after 15000 steps \tloss=0.00012653459762694135 neuron size 10\n",
            "9.171598782913514e-05\n",
            "after 20000 steps \tloss=8.816652507960177e-05 neuron size 10\n",
            "3.836807254733958e-05\n",
            "after 25000 steps \tloss=6.727763284153147e-05 neuron size 10\n",
            "2.08888922380703e-05\n",
            "after 30000 steps \tloss=5.420418740030591e-05 neuron size 10\n",
            "1.307344544122556e-05\n",
            "after 35000 steps \tloss=4.527910811658605e-05 neuron size 10\n",
            "8.925079283719862e-06\n",
            "after 40000 steps \tloss=3.881217687691189e-05 neuron size 10\n",
            "6.466931239674161e-06\n",
            "after 45000 steps \tloss=3.391847081860232e-05 neuron size 10\n",
            "4.893706058309565e-06\n",
            "after 50000 steps \tloss=3.0090751925806784e-05 neuron size 10\n",
            "3.8277188927955386e-06\n",
            "after 55000 steps \tloss=2.7017828119490874e-05 neuron size 10\n",
            "3.0729238063159094e-06\n",
            "after 60000 steps \tloss=2.4498399110216035e-05 neuron size 10\n",
            "2.519429009274839e-06\n",
            "after 65000 steps \tloss=2.2396598855182098e-05 neuron size 10\n",
            "2.101800255033937e-06\n",
            "after 70000 steps \tloss=2.0617472086042102e-05 neuron size 10\n",
            "1.7791267691399964e-06\n",
            "after 75000 steps \tloss=1.90926932478177e-05 neuron size 10\n",
            "1.5247788382244035e-06\n",
            "after 80000 steps \tloss=1.777186738769553e-05 neuron size 10\n",
            "1.3208258601221687e-06\n",
            "after 85000 steps \tloss=1.661702618384755e-05 neuron size 10\n",
            "1.154841203847979e-06\n",
            "after 90000 steps \tloss=1.559903056380136e-05 neuron size 10\n",
            "1.0179956200461921e-06\n",
            "after 95000 steps \tloss=1.469515238450218e-05 neuron size 10\n",
            "9.038781792991785e-07\n",
            "after 0 steps \tloss=1.8884855389160489 neuron size 20\n",
            "-0.8884855389160489\n",
            "after 5000 steps \tloss=0.0035979347760654564 neuron size 20\n",
            "1.8848876041399834\n",
            "after 10000 steps \tloss=0.0003771974161824121 neuron size 20\n",
            "0.0032207373598830444\n",
            "after 15000 steps \tloss=0.00017509592818472103 neuron size 20\n",
            "0.00020210148799769108\n",
            "after 20000 steps \tloss=0.00011087961776757217 neuron size 20\n",
            "6.421631041714886e-05\n",
            "after 25000 steps \tloss=8.020221393968896e-05 neuron size 20\n",
            "3.0677403827883206e-05\n",
            "after 30000 steps \tloss=6.244144454051691e-05 neuron size 20\n",
            "1.7760769399172052e-05\n",
            "after 35000 steps \tloss=5.093262409341595e-05 neuron size 20\n",
            "1.1508820447100963e-05\n",
            "after 40000 steps \tloss=4.290079633658014e-05 neuron size 20\n",
            "8.03182775683581e-06\n",
            "after 45000 steps \tloss=3.699296352036468e-05 neuron size 20\n",
            "5.9078328162154615e-06\n",
            "after 50000 steps \tloss=3.2473677562815756e-05 neuron size 20\n",
            "4.519285957548922e-06\n",
            "after 55000 steps \tloss=2.890999184528321e-05 neuron size 20\n",
            "3.5636857175325466e-06\n",
            "after 60000 steps \tloss=2.6030978948406645e-05 neuron size 20\n",
            "2.879012896876564e-06\n",
            "after 65000 steps \tloss=2.365869604022832e-05 neuron size 20\n",
            "2.3722829081783262e-06\n",
            "after 70000 steps \tloss=2.1671590103704606e-05 neuron size 20\n",
            "1.987105936523713e-06\n",
            "after 75000 steps \tloss=1.998388180755877e-05 neuron size 20\n",
            "1.687708296145835e-06\n",
            "after 80000 steps \tloss=1.8533355151341435e-05 neuron size 20\n",
            "1.450526656217336e-06\n",
            "after 85000 steps \tloss=1.7273816114010893e-05 neuron size 20\n",
            "1.2595390373305415e-06\n",
            "after 90000 steps \tloss=1.6170266030045814e-05 neuron size 20\n",
            "1.103550083965079e-06\n",
            "after 95000 steps \tloss=1.5195716213547273e-05 neuron size 20\n",
            "9.745498164985407e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5jLZNWNy-o-"
      },
      "source": [
        "\r\n",
        "#For Hidden layer with size equal to 2\r\n",
        "\r\n",
        "[[5.00029280e-01]\r\n",
        " [4.99968490e-01]\r\n",
        " [9.99974348e-01]\r\n",
        " [9.99974348e-01]\r\n",
        " [5.00025299e-01]\r\n",
        " [2.29279374e-05]\r\n",
        " [2.29279373e-05]\r\n",
        " [4.99982379e-01]]\r\n",
        "\r\n",
        "after 95000 steps \tloss=0.34661166374270724  \r\n",
        "\r\n",
        "\r\n",
        "Mimumum size to solve it but unreliably"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vdlYqCBzTi9"
      },
      "source": [
        "#For Hidden layer with size equal to 3\r\n",
        "\r\n",
        "\r\n",
        "[[7.74432386e-05]\r\n",
        " [9.99946786e-01]\r\n",
        " [9.99998127e-01]\r\n",
        " [9.99947160e-01]\r\n",
        " [5.98847338e-05]\r\n",
        " [1.52125907e-06]\r\n",
        " [5.91541861e-05]\r\n",
        " [9.99937405e-01]]\r\n",
        "after 95000 steps \tloss=4.606710834326667e-05\r\n",
        "\r\n",
        "Much better score, and it is the minimum size to solve it reliably"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaLEoV-9DLG"
      },
      "source": [
        "## Problem 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3lk9_TM-MvK"
      },
      "source": [
        "def ReLU(x):\r\n",
        "    return np.maximum(0, x)\r\n",
        "def sigmoid(x):\r\n",
        "    return 1/(1+ np.exp(-x))\r\n",
        "\r\n",
        "class SmallNetReLu:\r\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\r\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\r\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\r\n",
        "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\r\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\r\n",
        "        self.init_params()\r\n",
        "\r\n",
        "    def init_params(self):\r\n",
        "        # TODO for Problem 2:\r\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\r\n",
        "        self.W1 = np.random.uniform(low=-0.5, high=0.5, size=(self.W1.shape))\r\n",
        "        # print(self.W1, \"init_params\")\r\n",
        "        self.b1 = np.random.uniform(low=-0.5, high=0.5, size=(self.b1.shape))\r\n",
        "        self.W2 = np.random.uniform(low=-0.5, high=0.5, size=(self.W2.shape))\r\n",
        "        self.b2 = np.random.uniform(low=-0.5, high=0.5, size=(self.b2.shape))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, X, Y=None, do_backward=False):\r\n",
        "        # TODO Problem 1: Fill in details of forward propagation\r\n",
        "\r\n",
        "        # Input to neurons in 1st layer x W_1^T + b_1\r\n",
        "        A1 = np.dot(X, self.W1.T) + self.b1\r\n",
        "        # Outputs after the ReLU non-linearity\r\n",
        "        O1 = ReLU(A1)\r\n",
        "        A2 = np.add(np.dot(O1, self.W2.T), self.b2)  \r\n",
        "        O2 = sigmoid(A2)\r\n",
        "             \r\n",
        "\r\n",
        "        # When Y is none, simply return the predictions. Else compute the loss\r\n",
        "        if Y is not None:   \r\n",
        "            loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\r\n",
        "            # normalize loss by batch size\r\n",
        "            loss = loss.sum() / X.shape[0]\r\n",
        "            # print(loss)\r\n",
        "        else:\r\n",
        "            loss = np.nan\r\n",
        "\r\n",
        "        if do_backward:\r\n",
        "            \r\n",
        "            A2_grad = O2 - Y\r\n",
        "            # print(A2_grad, \"Wynik ostatniej aktywacji - dokładna wartosc\")       \r\n",
        "            self.b2_grad = A2_grad.sum(0) / A2.shape[0]\r\n",
        "            self.W2_grad = np.dot(A2_grad.T, O1) / A2.shape[0]\r\n",
        "            O1_grad = np.dot(A2_grad, self.W2)\r\n",
        "            tmp = np.zeros(O1.shape)\r\n",
        "            # print(tmp)\r\n",
        "            for i, v in enumerate(O1):\r\n",
        "               if v[0] > 0 and v[1] > 0:\r\n",
        "                    tmp[i] = 1\r\n",
        "            # print(tmp)        \r\n",
        "            A1_grad = O1_grad * tmp\r\n",
        "            self.b1_grad = A1_grad.sum(0) / A2.shape[0]\r\n",
        "            self.W1_grad = A1_grad.T @ X /A2.shape[0]\r\n",
        "\r\n",
        "        return O2, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwIsitQP0Nya",
        "outputId": "4930d6a9-99ba-4b94-cb2a-0fb57ffa63f0"
      },
      "source": [
        "X3 = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1]], dtype=np.float32)\r\n",
        "Y3 =  np.array([[0], [1], [1], [1], [0], [0], [0], [1]], dtype=np.float32)\r\n",
        "# [2, 3, 5, 10, 20]\r\n",
        "for hidden_dim in [2, 3, 5, 10, 20, 50, 100]:\r\n",
        "    # TODO: run a fe trainings and record the fraction of successful ones\r\n",
        "    net = SmallNetReLu(3, hidden_dim, dtype=np.float32)\r\n",
        "    asd, loss = net.forward(X3, Y3, do_backward=True)\r\n",
        "    alpha = 1 # set a learning rate\r\n",
        "    for i in range(100000):\r\n",
        "        asd, loss = net.forward(X3, Y3, do_backward=True)\r\n",
        "        if (i == 95000):\r\n",
        "            print(f\"after {i} steps \\tloss={loss}\", \"Number of hidden layer {}\".format(hidden_dim))\r\n",
        "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\"]:\r\n",
        "            param = getattr(net, param_name)\r\n",
        "            # print(param)\r\n",
        "            # Hint: use the construct `param[:]` to change the contents of the array!\r\n",
        "            # Doing instead `param = new_val` simply changes to what the variable\r\n",
        "            # param points to, without affecting the network!\r\n",
        "            # alternatively, you could do setattr(net, param_name, new_value)\r\n",
        "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after 95000 steps \tloss=0.602833100512351 Number of hidden layer 2\n",
            "after 95000 steps \tloss=0.5973930458686108 Number of hidden layer 3\n",
            "after 95000 steps \tloss=0.49856099883844673 Number of hidden layer 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "after 95000 steps \tloss=4.818412813521675 Number of hidden layer 10\n",
            "after 95000 steps \tloss=0.0039963902429707605 Number of hidden layer 20\n",
            "after 95000 steps \tloss=0.3206669600378373 Number of hidden layer 50\n",
            "after 95000 steps \tloss=0.000739878676224315 Number of hidden layer 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hr_iAKX-ND1"
      },
      "source": [
        "## Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnz6CndQ-NRI"
      },
      "source": [
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "def sigmoid(x):\n",
        "    return 1/(1+ np.exp(-x))\n",
        "\n",
        "class SmallNetReLu1:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((num_hidden, num_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W3 = np.zeros((1, num_hidden), dtype=dtype)\n",
        "        self.b3 = np.zeros((1,), dtype=dtype)\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        # TODO for Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.uniform(low=-0.1, high=0.5, size=(self.W1.shape))\n",
        "        # print(self.W1, \"init_params\")\n",
        "        self.b1 = np.random.uniform(low=-0.1, high=0.5, size=(self.b1.shape))\n",
        "        self.W2 = np.random.uniform(low=-0.1, high=0.5, size=(self.W2.shape))\n",
        "        self.b2 = np.random.uniform(low=-0.1, high=0.5, size=(self.b2.shape))\n",
        "        self.W3 = np.random.uniform(low=-0.1, high=0.5, size=(self.W3.shape))\n",
        "        self.b3 = np.random.uniform(low=-0.1, high=0.5, size=(self.b3.shape))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        \n",
        "\n",
        "        # Input to neurons in 1st layer x W_1^T + b_1\n",
        "        A1 = np.dot(X, self.W1.T) + self.b1\n",
        "        O1 = ReLU(A1)\n",
        "        A2 = np.dot(O1, self.W2.T) + self.b2\n",
        "        O2 = ReLU(A2)\n",
        "\n",
        "        A3 = np.add(np.dot(O2, self.W3.T), self.b3)  \n",
        "        O3 = sigmoid(A3)\n",
        "             \n",
        "\n",
        "        # When Y is none, simply return the predictions. Else compute the loss\n",
        "        if Y is not None:\n",
        "            loss = -Y * np.log(O2) - (1 - Y) * np.log(1.0 - O2)\n",
        "            # normalize loss by batch size\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "            # print(loss)\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "\n",
        "            A3_grad = O3 - Y   \n",
        "            self.b3_grad = A3_grad.sum(0) / A3.shape[0]\n",
        "            self.W3_grad = np.dot(A3_grad.T, O2) / A3.shape[0]\n",
        "            \n",
        "            O2_grad = np.dot(A3_grad, self.W3)\n",
        "            tmp = np.zeros(O2_grad.shape)\n",
        "            for i, v in enumerate(O2):\n",
        "               if v[0] > 0 and v[1] > 0:\n",
        "                    tmp[i] = 1\n",
        "         \n",
        "            A2_grad = O2_grad * tmp\n",
        "\n",
        "            self.b2_grad = A2_grad.sum(0) / A3.shape[0]\n",
        "            self.W2_grad = np.dot(A2_grad.T, O1)/ A3.shape[0]\n",
        "     \n",
        "            O1_grad = np.dot(A2_grad, self.W2)\n",
        "            \n",
        "            tmp = np.zeros(O1_grad.shape)\n",
        "           \n",
        "            for i, v in enumerate(O1):\n",
        "               if v[0] > 0 and v[1] > 0:\n",
        "                    tmp[i] = 1\n",
        "          \n",
        "           \n",
        "            A1_grad = O1_grad * tmp\n",
        "        \n",
        "            self.b1_grad = A1_grad.sum(0) / A3.shape[0]\n",
        "            self.W1_grad = A1_grad.T @ X /A3.shape[0]\n",
        "            # print(self.W1_grad.shape)\n",
        "            # print(self.W1.shape)\n",
        "            # print(self.b1_grad.shape)\n",
        "            # print(self.b1.shape)\n",
        "\n",
        "        return O3, loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPu-H7nIYNWL",
        "outputId": "07cfa500-3279-4983-c214-d7e1eff70b96"
      },
      "source": [
        "X3 = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0], [1, 1, 0], [1, 0, 1], [0, 1, 1], [1, 1, 1]], dtype=np.float32)\n",
        "Y3 =  np.array([[0], [1], [1], [1], [0], [0], [0], [1]], dtype=np.float32)\n",
        "[2, 3, 5, 10, 20]\n",
        "\n",
        "\n",
        "for hidden_dim in [2, 3, 5, 10, 20, 50]:\n",
        "    # TODO: run a fe trainings and record the fraction of successful ones\n",
        "    net = SmallNetReLu1(3, hidden_dim, dtype=np.float32)\n",
        "    asd, loss = net.forward(X3, Y3, do_backward=True)\n",
        "\n",
        "    alpha = 0.1 # set a learning rate\n",
        "    for i in range(100000):\n",
        "        asd, loss = net.forward(X3, Y3, do_backward=True)\n",
        "        if (i == 95000):\n",
        "            print(f\"after {i} steps \\tloss={loss}\", \"Number of hidden layer {}\".format(hidden_dim))\n",
        "            # \n",
        "        for param_name in [\"W1\", \"b1\", \"W2\", \"b2\",\"W3\", \"b3\"]:\n",
        "            param = getattr(net, param_name)\n",
        "            # print(param)\n",
        "            # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "            # Doing instead `param = new_val` simply changes to what the variable\n",
        "            # param points to, without affecting the network!\n",
        "            # alternatively, you could do setattr(net, param_name, new_value)\n",
        "            param[:] = param - alpha * getattr(net, param_name + \"_grad\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in multiply\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: RuntimeWarning: invalid value encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "after 95000 steps \tloss=nan Number of hidden layer 2\n",
            "after 95000 steps \tloss=2.349661319459032 Number of hidden layer 3\n",
            "after 95000 steps \tloss=nan Number of hidden layer 5\n",
            "after 95000 steps \tloss=nan Number of hidden layer 10\n",
            "after 95000 steps \tloss=nan Number of hidden layer 20\n",
            "after 95000 steps \tloss=nan Number of hidden layer 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB3VcHlbnUZ_"
      },
      "source": [
        "loss = nan but predictions are good\n",
        "\n",
        "[[0.24678714]\n",
        " [0.74523735]\n",
        " [0.25004536]\n",
        " [0.75844653]\n",
        " [0.24644634]\n",
        " [0.74992847]\n",
        " [0.25672116]\n",
        " [0.74638765]]\n",
        "after 95000 steps \tloss=nan Number of hidden layer 2\n",
        "[[0.5]\n",
        " [0.5]\n",
        " [0.5]\n",
        " [0.5]\n",
        " [0.5]\n",
        " [0.5]\n",
        " [0.5]\n",
        " [0.5]]\n",
        "after 95000 steps \tloss=2.2526904739320024 Number of hidden layer 3\n",
        "[[2.34022960e-04]\n",
        " [9.99085103e-01]\n",
        " [9.97676367e-01]\n",
        " [9.99500093e-01]\n",
        " [1.74984040e-04]\n",
        " [1.04891670e-04]\n",
        " [6.75124794e-03]\n",
        " [9.93633863e-01]]\n",
        "after 95000 steps \tloss=nan Number of hidden layer 5\n",
        "[[0.5002551 ]\n",
        " [0.50413557]\n",
        " [0.49793582]\n",
        " [0.49770025]\n",
        " [0.50410883]\n",
        " [0.49790908]\n",
        " [0.49767352]\n",
        " [0.50028183]]\n",
        "after 95000 steps \tloss=nan Number of hidden layer 10\n",
        "[[0.03408587]\n",
        " [0.96478616]\n",
        " [0.98654236]\n",
        " [0.95334973]\n",
        " [0.03673089]\n",
        " [0.06375009]\n",
        " [0.00471075]\n",
        " [0.95636956]]\n",
        "after 95000 steps \tloss=nan Number of hidden layer 20\n",
        "[[5.45075188e-02]\n",
        " [9.88872384e-01]\n",
        " [9.74969044e-01]\n",
        " [9.67603044e-01]\n",
        " [5.07355789e-04]\n",
        " [1.38797318e-02]\n",
        " [1.09668318e-02]\n",
        " [9.90580234e-01]]\n",
        "after 95000 steps \tloss=nan Number of hidden layer 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PcNxrCt-NcN"
      },
      "source": [
        "## Problem 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Brepirl-Nln"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWuv7Q77-Nut"
      },
      "source": [
        "## Problem 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avuvSoWY-N4Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}